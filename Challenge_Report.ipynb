{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge_Report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWf9JEFScKJX",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction\n",
        "Sleep is crucial for individuals' health and well being, it progresses in cycles that involve multiple sleep stages : wake, light sleep, deep sleep, rem sleep and the monitoring of these stages is useful for detecting sleep disorders.\\\n",
        "The Dreem headband offers a solution to perform a polysomnography -which consists of the recording of different physiological signals such as electroencephalogram, electrocardiogram etc.- at home.\\\n",
        "In this challenge, we suggest methods to predict sleep stages (labeled from 0 to 4) on windows of 30 seconds of raw data based on the corresponding recorded signals. The raw data provided by the three sensors on the headband includes 7 eegs channels in frontal and occipital position, 1 pulse oximeter infrared channel, and 3 accelerometers channels (x, y and z). We manage to achieve a $F_1$ score of 0.764.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06pTnzPBZtGg",
        "colab_type": "text"
      },
      "source": [
        "# 2. Features extraction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WANc1dsuaD6S",
        "colab_type": "text"
      },
      "source": [
        "In order to construct a design matrix for our machine learning processing, we extract several statistical features from from our eeg signals denoted $x = (x_{1},...x_{n})$. The extracted features are the following :\n",
        "\n",
        "**Mean** :\n",
        "\n",
        "For each eeg signal we compute :\n",
        "$$ \\bar{x} = \\sum_{i = 1}^{n} \\frac{1}{n} x_{i}$$\n",
        "**Standard deviation**:\n",
        "\n",
        "$$ \\sigma(x) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n",
        "\n",
        "**Percentiles of the data**:\n",
        "\n",
        "Without loss of generality, suppose that we sorted the points of the signal so that $x_{1}$ is the smallest value and $x_{n}$ is the greatest value. $x_i$ is the $q_i$ percentile of the signal where :\n",
        "$$  q_i = 100\\frac{i-0.5}{n}$$\n",
        "The 5th, 25th, 50th (median), 75th and 95th are all computed for each signal.\n",
        "\n",
        "**Number of zero crossing**:\n",
        "\n",
        "we count the number of times at which the value of signal is zero.\n",
        "$$ zeros(x) = \\sum_{i = 1}^{n} \\delta_{x_i, 0}$$\n",
        "**Number of mean crossing** :\n",
        "\n",
        "we count the number of times at which the value of signal is the mean of the signal.\n",
        "$$ zeros(x) = \\sum_{i = 1}^{n} \\delta_{x_i, \\bar{x}}$$\n",
        "\n",
        "**Root Mean Square** :\n",
        "\n",
        "$$ rms(x) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^2}$$\n",
        "\n",
        "**Absolute energy** :\n",
        "$$ abs(x) = \\sum_{i =1}^{n}x_{i}^2 $$\n",
        "** Inter-quartile range** :\n",
        "\n",
        "we substract the first quartile from the third quartile :\n",
        "$$ IQR = q_75 - q_25 $$\n",
        "\n",
        "**Median absolute deviation** :\n",
        "\n",
        "$$ MAD(x) = Median(| x - Median(x)|)$$\n",
        "**Shanon entropy** :\n",
        "\n",
        "In general, we can define the entropy of a discrete random variables $X$ with values in ${x_{1},...x_{n}}$ as follows :\n",
        "$$ H(X) = -c\\sum_{i = 1}^{n} p(x_{i})ln(p(x_{i}))$$\n",
        "$ p(x_{i}) $ is the probability of occurrence of $x_{i}$.\n",
        "In practice we take $c = 1$ and $ p(x_{i}) $ as the empirical occurrence frequency of $x_{i}$.\n",
        "\n",
        "**Spectral entropy** : \n",
        "\n",
        "Given a sampling frequency $f_s$, The spectral entropy is nothing but the  Shanon entropy of the power spectral density of the signal PSD (Normalized).\n",
        "$$ H(x,f_s) = -\\sum_{ f = 0}^{f_s/2} PSD(f)ln(PSD(f))  $$\n",
        "\n",
        "**Petrosian fractal dimension** :\n",
        "\n",
        "$$ PFD = \\frac{Log(n)}{Log(n) + \\frac{Log(n)}{Log(n)+ 0.4N_{\\Delta}}} $$\n",
        "with : \n",
        "$$ N_{\\Delta} = \\frac{1}{2} \\sum_{i = 1}^{n-1} 1 -sign(x_{i}x_{i+1})$$\n",
        "\n",
        "**Hjorth parameters** :\n",
        "\n",
        "Hjorth parameters consists in three statistical properties in the time domain.\n",
        "\n",
        "**Activity** :\n",
        "\n",
        "The activity is nothing but the variance of the signal \n",
        "$$ activity(x) = var(x(t)) $$\n",
        "\n",
        "**Morbidity**:\n",
        "\n",
        "The morbidity is a way to capture the mean frequency in the power spectrum :\n",
        "$$ morbidity(x) = \\sqrt{\\frac{var(\\frac{dx}{dt})}{var(x)}}$$\n",
        "\n",
        "**Complexity** :\n",
        "\n",
        "The complexity is a way to capture the change in frequency and is defined by the mean of the the mobility.\n",
        "$$ complexity(x) = \\frac{mobility(\\frac{dx}{dt})}{morbidity(x)}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29oGl4aeXQfU",
        "colab_type": "text"
      },
      "source": [
        "# 3. Construction of the design matrix : Python Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De8zAjpPcYwy",
        "colab_type": "text"
      },
      "source": [
        "All the features are implemented in python. For that purpose, we define several functions that extract features for each signal. We regroup the straightforward standard statistcs in one function called *calculate_statistics(list_values)*, we group in another function called *calculate_crossings(list_values)* the zero and mean crossing. For the rest, each feature is defined as an independent function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkIWDHgPco8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numba import jit\n",
        "from math import factorial, log\n",
        "from sklearn.neighbors import KDTree\n",
        "from scipy.signal import periodogram, welch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import scipy.stats\n",
        "import numpy as np \n",
        "\"\"\"...................................................Hjorth parameters..................................................\"\"\"\n",
        "def hjorth(a):\n",
        "    first_deriv = np.diff(a)\n",
        "    second_deriv = np.diff(a,2)\n",
        "\n",
        "    var_zero = np.mean(a ** 2)\n",
        "    var_d1 = np.mean(first_deriv ** 2)\n",
        "    var_d2 = np.mean(second_deriv ** 2)\n",
        "\n",
        "    activity = var_zero\n",
        "    morbidity = np.sqrt(var_d1 / var_zero)\n",
        "    complexity = np.sqrt(var_d2 / var_d1) / morbidity\n",
        "\n",
        "    return activity, morbidity, complexity\n",
        "    import pandas as pd\n",
        "\n",
        "\"\"\"...............................................Spectral_Entropy.................................................\"\"\"\n",
        "\n",
        "def spectral_entropy(x, sf, method='fft', nperseg=None, normalize=False):\n",
        "    x = np.array(x)\n",
        "    # Compute and normalize power spectrum\n",
        "    if method == 'fft':\n",
        "        _, psd = periodogram(x, sf)\n",
        "    elif method == 'welch':\n",
        "        _, psd = welch(x, sf, nperseg=nperseg)\n",
        "    # add cond\n",
        "    if psd.sum() == 0 :\n",
        "        return np.nan\n",
        "    psd_norm = np.divide(psd, psd.sum())\n",
        "    se = -np.multiply(psd_norm, np.log2(psd_norm)).sum()\n",
        "    if normalize:\n",
        "        if psd_norm.size == 0:\n",
        "            return np.nan\n",
        "        se /= np.log2(psd_norm.size)\n",
        "    return se\n",
        "\"\"\"................................................Petrosian Fractal Diemension............................................\"\"\"\n",
        "def petrosian_fd(x):\n",
        "    n = len(x)\n",
        "    # Number of sign changes in the first derivative of the signal\n",
        "    diff = np.ediff1d(x)\n",
        "    N_delta = (diff[1:-1] * diff[0:-2] < 0).sum()\n",
        "    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n",
        "\n",
        "\"\"\"..........................................  Absolute energy  ..............................................\"\"\"\n",
        "def abs_energy(x):\n",
        "    if not isinstance(x, (np.ndarray, pd.Series)):\n",
        "        x = np.asarray(x)\n",
        "    return np.dot(x, x)\n",
        "\n",
        "\"\"\"..........................................  Inter_Q_Range................................................\"\"\"\n",
        "from scipy import stats\n",
        "def inter_q_range(x):\n",
        "  return stats.iqr(x)\n",
        "\"\"\"..........................................median_absolute_deviation.............................................\"\"\"\n",
        "def median_absolute_deviation(data, axis=None, func=None, ignore_nan=False):\n",
        "    if func is None:\n",
        "        # Check if the array has a mask and if so use np.ma.median\n",
        "        # See https://github.com/numpy/numpy/issues/7330 why using np.ma.median\n",
        "        # for normal arrays should not be done (summary: np.ma.median always\n",
        "        # returns an masked array even if the result should be scalar). (#4658)\n",
        "        if isinstance(data, np.ma.MaskedArray):\n",
        "            is_masked = True\n",
        "            func = np.ma.median\n",
        "            if ignore_nan:\n",
        "                data = np.ma.masked_where(np.isnan(data), data, copy=False)\n",
        "        elif ignore_nan:\n",
        "            is_masked = False\n",
        "            func = np.nanmedian\n",
        "        else:\n",
        "            is_masked = False\n",
        "            func = np.median\n",
        "    else:\n",
        "        is_masked = None\n",
        "\n",
        "    data = np.asanyarray(data)\n",
        "    # np.nanmedian has `keepdims`, which is a good option if we're not allowing\n",
        "    # user-passed functions here\n",
        "    data_median = func(data, axis=axis)\n",
        "\n",
        "    # broadcast the median array before subtraction\n",
        "    if axis is not None:\n",
        "        if isiterable(axis):\n",
        "            for ax in sorted(list(axis)):\n",
        "                data_median = np.expand_dims(data_median, axis=ax)\n",
        "        else:\n",
        "            data_median = np.expand_dims(data_median, axis=axis)\n",
        "\n",
        "    result = func(np.abs(data - data_median), axis=axis, overwrite_input=True)\n",
        "\n",
        "    if axis is None and np.ma.isMaskedArray(result):\n",
        "        # return scalar version\n",
        "        result = result.item()\n",
        "    elif np.ma.isMaskedArray(result) and not is_masked:\n",
        "        # if the input array was not a masked array, we don't want to return a\n",
        "        # masked array\n",
        "        result = result.filled(fill_value=np.nan)\n",
        "\n",
        "    return result\n",
        "\"\"\"..................................................... Shanon entopy...............................................................\"\"\"\n",
        "def calculate_entropy(list_values):\n",
        "    counter_values = Counter(list_values).most_common()\n",
        "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
        "    entropy = scipy.stats.entropy(probabilities)\n",
        "    return entropy\n",
        "\"\"\"..................................................... Standard statistics .......................................................\"\"\"\n",
        "def calculate_statistics(list_values):\n",
        "    n5 = np.nanpercentile(list_values, 5)\n",
        "    n25 = np.nanpercentile(list_values, 25)\n",
        "    n75 = np.nanpercentile(list_values, 75)\n",
        "    n95 = np.nanpercentile(list_values, 95)\n",
        "    median = np.nanpercentile(list_values, 50)\n",
        "    mean = np.mean(list_values)\n",
        "    std = np.nanstd(list_values)\n",
        "    var = np.nanvar(list_values)\n",
        "    rms = np.nanmean(np.sqrt(list_values**2))\n",
        "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
        "\"\"\"..................................................... Crossings........... .......................................................\"\"\" \n",
        "def calculate_crossings(list_values):\n",
        "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
        "    no_zero_crossings = len(zero_crossing_indices)\n",
        "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
        "    no_mean_crossings = len(mean_crossing_indices)\n",
        "    return [no_zero_crossings, no_mean_crossings]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqHCCR1BgNu-",
        "colab_type": "text"
      },
      "source": [
        "We now define a function called *get_features(list_values)* that returns for each signal a list of the computed features i.e the associated line in the design matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzslDkTCgMrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import pywt\n",
        "def get_features(list_values):\n",
        "    a,b,c = hjorth(list_values)\n",
        "    d = spectral_entropy(list_values, sf = 240)\n",
        "    e = petrosian_fd(list_values)\n",
        "    f = abs_energy(list_values)\n",
        "    g = inter_q_range(list_values)\n",
        "    h = median_absolute_deviation(list_values)\n",
        "    entropy = calculate_entropy(list_values)\n",
        "    crossings = calculate_crossings(list_values)\n",
        "    statistics = calculate_statistics(list_values)\n",
        "    return [entropy] + crossings + statistics + [a,b,c,d,e,f,g,h]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIAaFTHSjCmC",
        "colab_type": "text"
      },
      "source": [
        "The very next step is to construct the design matrix for both the training and test data. We will restrict ourself for only one eeg channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3wl1V_wd46r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"/content/drive/My Drive/X_train.h5\", 'r') as f: \n",
        "  eeg_1 = f['eeg_1']\n",
        "  n,p   = eeg_1.shape \n",
        "  list_fesature = []\n",
        "  for k in range(n) :\n",
        "    features = []\n",
        "    features += get_features(eeg_1[k])\n",
        "    list_features.append(features)\n",
        "  X_train = np.array(X_train)\n",
        "  np.savetxt('X_train.txt', X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI2F77BIoH1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"/content/drive/My Drive/X_test.h5\", 'r') as f: \n",
        "  eeg_1 = f['eeg_1']\n",
        "  n,p   = eeg_1.shape \n",
        "  list_fesature = []\n",
        "  for k in range(n) :\n",
        "    features = []\n",
        "    features += get_features(eeg_1[k])\n",
        "    list_features.append(features)\n",
        "  X_test = np.array(X_test)\n",
        "  np.savetxt('X_train.txt', X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukNpHUSrcHt",
        "colab_type": "text"
      },
      "source": [
        "We download the labels of the training data and we transform it to a pure of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdbvBQR5rWFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "y = pd.read_csv(\"y_train.csv\", sep= ',')\n",
        "Y_train= []\n",
        "for i in range(24684) :\n",
        "  Y_train = Y_train + [y['sleep_stage'][i]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh2Lvj0tr9-6",
        "colab_type": "text"
      },
      "source": [
        "Fro the training data we remove some lines for which the we don't have labels. As for None or inf values, we replace them by zero in both the training and the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7REuIKrXOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.delete(X_train, obj= [i for i in range(24684, 24688)], axis =0)\n",
        "X_train = np.nan_to_num(X_train)\n",
        "X_test = np.nan_to_num(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJmC8NusaL3g",
        "colab_type": "text"
      },
      "source": [
        "# 4. Algorithms \n",
        "Among the tested alhorithms, we restrict our discussion to the two most promising algorithms : Gradient Boosting classifier and Decision Tree with a Bagging classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZKOhdIwB50",
        "colab_type": "text"
      },
      "source": [
        "We run both of the algorithms without any tuninig of parameters. Most of the parameters keep their default values. The objective here is purely exploratory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWzsYDEZtNf3",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Gradient Boosting classifier \n",
        "This algorithm consists in filtering sequentially observations to focus each time on developping a new learner that could deal with the difficult observations. The wealk learners take the form of the decision trees wich are constructed by making the best split accprding eith a purity score or a loss minimization. At each time a tree is added, a gradient decent procedure is used to min the loss function. The following code runs the algorithm on our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wd6Kafht4P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(  n_estimators= 100 )\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_predict = clf.predict(X_test) \n",
        "Prediction = {'index' : [i + 24688 for i in range(len(Y_predict))], 'sleep_stage': Y_predict  }\n",
        "Prediction = pd.DataFrame(Prediction , columns= [ 'index', 'sleep_stage'])\n",
        "Prediction.to_csv (r'Predict_GradBoostMergeplus7ch08.csv', index = False ,header=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Ku_kuvb9NY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaG7m2W-t6ze",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Decision Tree with a Bagging classifier.\n",
        "The bagging is an application of the bootstrap procedure to high-variance alhorithms. the training data is resampled into independent samples using for example an uniform sampling distribution. Thus, each model in the ensemble votes with equal weight. Each model in the ensemble is trained using a randomly drawn subset of the training set. The models are fitted using the created samples and are combined using voting (classification problem ). The decision tree algorithm can be enhanced when combined with a bagging classifier. The code below is an implementation of the latter procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5IzNlDQuBsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(criterion = \"entropy\")\n",
        "bag_model = BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)\n",
        "bag_model = bag_model.fit(X_train, Y_train)\n",
        "Y_predict = bag_model.predict(X_test)\n",
        "Prediction = {'index' : [i + 24688 for i in range(len(Y_predict))], 'sleep_stage': Y_predict  }\n",
        "Prediction = pd.DataFrame( Prediction, columns = [ 'index', 'sleep_stage']) \n",
        "Prediction.to_csv (r'Predict_BaggMergeplusentropy3ch.csv', index = False ,header=True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NRHGl19X8tQ",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Results : Single eeg channel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM7sGVWKwd5G",
        "colab_type": "text"
      },
      "source": [
        "The resulted $F_{1}$ scores are for each algorithm : \n",
        "*   Gradient Boosting classifier : 0.51\n",
        "*   Decision Tree with a Bagging classifier : 0.58.\n",
        "\n",
        "For the given design matrix, the Decision Tree with a Bagging classifier appears to be better. Our next move is try to redisign our matrix in a way that improves the scores without any hyperparameters tuning. In the following, we add some features related to the frequency domain of the signals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbQxwZyyBd3",
        "colab_type": "text"
      },
      "source": [
        "# 5. Features from the frequency domain.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20FIL9XsYvem",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Signal transforms and their Python implementation\n",
        "Features in the frequency domain are going to be essentially the highest peaks given by the signal. In this way, each new feature corresponds to a peak of the peak of the signal. The transformation of the signal to the frequency domain is ensured by three techniques :\n",
        "\n",
        "\n",
        "*   Fast Fouriers Transform (FFT)\n",
        "*   Power spectral density (PSD)\n",
        "*   Autocorrelation function (Autocorr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2lmaz6D3mvw",
        "colab_type": "text"
      },
      "source": [
        "To determine the peaks of a given signal, we will use a function *detect_peaks* suggested by *Marcos Duarte* which one can find on github. This function returns the peaks on amplitude or other criterions. For our usage, We only detect peaks based on their amplitude. Our obejctive is detect only the highest five peaks for each transformation. *detect_peaks* will provide us each time with all the peaks greater than minimium hight set to be the 10% of the maximum hight. This condition is onlu an empirical way to ensure the existence of five peaks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kba0lC0w3hzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Detect peaks in data based on their amplitude and other features.\"\"\"\n",
        "\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "\n",
        "__author__ = \"Marcos Duarte, https://github.com/demotu/BMC\"\n",
        "__version__ = \"1.0.6\"\n",
        "__license__ = \"MIT\"\n",
        "\n",
        "\n",
        "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
        "                 kpsh=False, valley=False, show=False, ax=None, title=True):\n",
        "\n",
        "    \"\"\"Detect peaks in data based on their amplitude and other features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 1D array_like\n",
        "        data.\n",
        "    mph : {None, number}, optional (default = None)\n",
        "        detect peaks that are greater than minimum peak height (if parameter\n",
        "        `valley` is False) or peaks that are smaller than maximum peak height\n",
        "         (if parameter `valley` is True).\n",
        "    mpd : positive integer, optional (default = 1)\n",
        "        detect peaks that are at least separated by minimum peak distance (in\n",
        "        number of data).\n",
        "    threshold : positive number, optional (default = 0)\n",
        "        detect peaks (valleys) that are greater (smaller) than `threshold`\n",
        "        in relation to their immediate neighbors.\n",
        "    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n",
        "        for a flat peak, keep only the rising edge ('rising'), only the\n",
        "        falling edge ('falling'), both edges ('both'), or don't detect a\n",
        "        flat peak (None).\n",
        "    kpsh : bool, optional (default = False)\n",
        "        keep peaks with same height even if they are closer than `mpd`.\n",
        "    valley : bool, optional (default = False)\n",
        "        if True (1), detect valleys (local minima) instead of peaks.\n",
        "    show : bool, optional (default = False)\n",
        "        if True (1), plot data in matplotlib figure.\n",
        "    ax : a matplotlib.axes.Axes instance, optional (default = None).\n",
        "    title : bool or string, optional (default = True)\n",
        "        if True, show standard title. If False or empty string, doesn't show\n",
        "        any title. If string, shows string as title.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ind : 1D array_like\n",
        "        indeces of the peaks in `x`.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The detection of valleys instead of peaks is performed internally by simply\n",
        "    negating the data: `ind_valleys = detect_peaks(-x)`\n",
        "\n",
        "    The function can handle NaN's\n",
        "\n",
        "    See this IPython Notebook [1]_.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
        "    \"\"\"\n",
        "\n",
        "    x = np.atleast_1d(x).astype('float64')\n",
        "    if x.size < 3:\n",
        "        return np.array([], dtype=int)\n",
        "    if valley:\n",
        "        x = -x\n",
        "        if mph is not None:\n",
        "            mph = -mph\n",
        "    # find indices of all peaks\n",
        "    dx = x[1:] - x[:-1]\n",
        "    # handle NaN's\n",
        "    indnan = np.where(np.isnan(x))[0]\n",
        "    if indnan.size:\n",
        "        x[indnan] = np.inf\n",
        "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
        "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
        "    if not edge:\n",
        "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
        "    else:\n",
        "        if edge.lower() in ['rising', 'both']:\n",
        "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
        "        if edge.lower() in ['falling', 'both']:\n",
        "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
        "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
        "    # handle NaN's\n",
        "    if ind.size and indnan.size:\n",
        "        # NaN's and values close to NaN's cannot be peaks\n",
        "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
        "    # first and last values of x cannot be peaks\n",
        "    if ind.size and ind[0] == 0:\n",
        "        ind = ind[1:]\n",
        "    if ind.size and ind[-1] == x.size-1:\n",
        "        ind = ind[:-1]\n",
        "    # remove peaks < minimum peak height\n",
        "    if ind.size and mph is not None:\n",
        "        ind = ind[x[ind] >= mph]\n",
        "    # remove peaks - neighbors < threshold\n",
        "    if ind.size and threshold > 0:\n",
        "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
        "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
        "    # detect small peaks closer than minimum peak distance\n",
        "    if ind.size and mpd > 1:\n",
        "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
        "        idel = np.zeros(ind.size, dtype=bool)\n",
        "        for i in range(ind.size):\n",
        "            if not idel[i]:\n",
        "                # keep peaks with the same height if kpsh is True\n",
        "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
        "                       & (x[ind[i]] > x[ind] if kpsh else True)\n",
        "                idel[i] = 0  # Keep current peak\n",
        "        # remove the small peaks and sort back the indices by their occurrence\n",
        "        ind = np.sort(ind[~idel])\n",
        "\n",
        "    if show:\n",
        "        if indnan.size:\n",
        "            x[indnan] = np.nan\n",
        "        if valley:\n",
        "            x = -x\n",
        "            if mph is not None:\n",
        "                mph = -mph\n",
        "        _plot(x, mph, mpd, threshold, edge, valley, ax, ind, title)\n",
        "\n",
        "    return ind\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nakuzMvDDvfq",
        "colab_type": "text"
      },
      "source": [
        "The following two functions ensure returning for a given signal transformation the x-values and y-values of the five peaks. We thus have extrcat ten features from each transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXGvgq_0vs39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_first_n_peaks(x,y,no_peaks=5):\n",
        "    x_, y_ = list(x), list(y)\n",
        "    if len(x_) >= no_peaks:\n",
        "        return x_[:no_peaks], y_[:no_peaks]\n",
        "    else:\n",
        "        missing_no_peaks = no_peaks-len(x_)\n",
        "        return x_ + [0]*missing_no_peaks, y_ + [0]*missing_no_peaks\n",
        "    \n",
        "def get_features(x_values, y_values, mph):\n",
        "    indices_peaks = detect_peaks(y_values, mph=mph)\n",
        "    peaks_x, peaks_y = get_first_n_peaks(x_values[indices_peaks], y_values[indices_peaks])\n",
        "    return peaks_x + peaks_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi6plvqQEdqI",
        "colab_type": "text"
      },
      "source": [
        "We implement below the three considered transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ZulyqbErbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_psd_values(y_values, T, N, f_s):\n",
        "    f_values, psd_values = welch(y_values, fs=f_s) # return freq values and ampl\n",
        "    return f_values, psd_values\n",
        "\n",
        "def get_fft_values(y_values, T, N, f_s):\n",
        "    f_values = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
        "    fft_values_ = fft(y_values)\n",
        "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
        "    return f_values, fft_values\n",
        "def autocorr(x):\n",
        "    result = np.correlate(x, x, mode='full')\n",
        "    return result[len(result)//2:]\n",
        " \n",
        "def get_autocorr_values(y_values, T, N, f_s):\n",
        "    autocorr_values = autocorr(y_values)\n",
        "    x_values = np.array([T * jj for jj in range(0, N)])\n",
        "    return x_values, autocorr_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2nDFi5RFQKa",
        "colab_type": "text"
      },
      "source": [
        "We now extract the additional features for the training and the test data. Based on similarity, it suffices to present the code applied to the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5DBQ6Q5FYRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 1500\n",
        "f_s = 240 # sampling freq\n",
        "T = 1/f_s\n",
        "with h5py.File(\"/content/drive/My Drive/X_train.h5\", 'r') as f: \n",
        "  eeg_i = f['eeg_' + str(1)]\n",
        "  n,p   = eeg_i.shape \n",
        "  Data = []\n",
        "  for k in range(n):\n",
        "    # Apply PSD\n",
        "    x_values, y_values = get_psd_values(eeg_i[k], T, N, f_s)\n",
        "    max_peak_height = 0.1 * np.nanmax(y_values)\n",
        "    PSD = get_features(x_values, y_values, mph = max_peak_height)\n",
        "    # Apply FFT\n",
        "    x_values, y_values = get_fft_values(eeg_i[k], T, N, f_s)\n",
        "    max_peak_height = 0.1 * np.nanmax(y_values)\n",
        "    FFT = get_features(x_values, y_values, mph = max_peak_height)\n",
        "    # Autocorr\n",
        "    x_values, y_values = get_autocorr_values(eeg_i[k], T, N, f_s)\n",
        "    max_peak_height = 0.1 * np.nanmax(y_values)\n",
        "    Autocorr = get_features(x_values, y_values, mph = max_peak_height)\n",
        "    Features = FFT + PSD + Autocorr\n",
        "    Data = Data + [Features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxt3ih4KGD-9",
        "colab_type": "text"
      },
      "source": [
        "The additiona features are simply added as new columns in our initial design matrix. We apply the same two algorithms to our new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXP0uNXkYeOh",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Results : One single eeg channel\n",
        "The resulted $F_{1}$ scores are for each algorithm : \n",
        "*   Gradient Boosting classifier : 0.53\n",
        "*   Decision Tree with a Bagging classifier : 0.61.\n",
        "We observe an improvement of the score compared to the initial ones but not very sensitive. The Decision Tree with a Bagging classifier is still better at this step.\n",
        "The fact that the added features did not improve the scores in a spectacualr way is mailnu due to the fact the esential tool of transformation -The Fourier transform- requires the frequency domain to be stationary. Unfortunately, the eeg signals appear to be dyanmic in the litterature. We thus need a tool that could provide simulataneously with the interesting frequencies and the time resolution at which they occur. Our next step is to perform wavelet transforms to our eeg signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9U_ESzcFiVv",
        "colab_type": "text"
      },
      "source": [
        "# 6. Wavelet transform and features extraction.\n",
        "The Fourier Transform has a high resolution in the frequency-domain but zero resolution in the time-domain. This means that it can tell us exactly which frequencies are present in a signal, but not at which location in time these frequencies have occurred. \n",
        "A better approach for analyzing signals with a dynamical frequency spectrum is the Wavelet Transform which has a high resolution in both the frequency- and the time-domain. It does not only tell us which frequencies are present in a signal, but also at which time these frequencies have occurred. This is accomplished by working with different scales. First, we look at the signal with a large scale/window and analyze ‘large’ features and then we look at the signal with smaller scales in order to analyze smaller features.\n",
        "Wavelets are mathematical functions that resemble the shape of wave oscillations, with the constraint for waves to start at 0 and then oscillate to 0 in the end. Mathematically, wavelets are described using two types of functions: the wavelet function (the mother function denoted with ψ(t)) and the scaling function (the father function denoted with φ(t)). These functions have to satisfy some conditions of integribality which we wills skip here.\n",
        "\n",
        "The application of the wavelet transform (in our case the discrete ) on the signal results in frequency sub-bands that capture some characteristics of the signal. For each sub-bands, we will extract the same features defined fro the initial design matrix.The discrete wavelet transforms used are : db1, db2, db3,db4 and haar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVCLzQ13rfmP",
        "colab_type": "text"
      },
      "source": [
        "In the following, we define a function that generate features for each frequency sub-bands given a wavelet tranform. *eeg_data* reprsents here the list of all the signals. This function is applied to both training signal and test signals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Ig3aHdrePz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_eeg_features(eeg_data,waveletname):\n",
        "    list_features = []\n",
        "    for i, signal in enumerate(eeg_data):\n",
        "        # print(i)\n",
        "        list_coeff = pywt.wavedec(signal, waveletname)\n",
        "        features = []\n",
        "        for coeff in list_coeff:\n",
        "            features += get_features(coeff)\n",
        "        list_features.append(features)\n",
        "    return list_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEzy_8z4s3Vr",
        "colab_type": "text"
      },
      "source": [
        "An example of features extraction is given below for one eeg channel. We use the wavelet db3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlLfFJyiGDU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"/content/drive/My Drive/X_train.h5\", 'r') as f: \n",
        "  eeg_1 = f['eeg_1']\n",
        "  n,p   = eeg_1.shape \n",
        "  X_train = get_eeg_features(eeg_data = eeg_1 ,waveletname = 'db3')\n",
        "  X_train = np.array(X_train)\n",
        "  np.savetxt('X_train_eeg1_db3.txt', X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYJZsjwbZr5M",
        "colab_type": "text"
      },
      "source": [
        "## 6.2 Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhprEtkZtQNb",
        "colab_type": "text"
      },
      "source": [
        "### 6.2.1 : Application of a single wavelet transform to a single eeg channel\n",
        "Once our training and test design matrices are constructed we run the same two algorithms. We apply seperately the wavelets db1 and db3.\n",
        "\n",
        "The resulted $F_{1}$ scores are for each algorithm : \n",
        "\n",
        "**For db1 :**\n",
        "*   Gradient Boosting classifier : 0.64\n",
        "*   Decision Tree with a Bagging classifier : 0.62.\n",
        "\n",
        "**For db3 :**\n",
        "*   Gradient Boosting classifier : 0.66\n",
        "*   Decision Tree with a Bagging classifier : 0.64.\n",
        "\n",
        "Using the wavelet transform, the improvement of the scores is quite sensitive. the Gradient Boosting classifier starts take the lead but the the resulted depens on the wavelet transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydnyuDzvpao",
        "colab_type": "text"
      },
      "source": [
        "### 6.2.2 Application of mutilple wavelet transforms to a single eeg channel.\n",
        "\n",
        "In this step we suggest to apply the five considered wavelet transforms and cancatenate vertically the resulted columns in the design matrix. We will restrict ourself to one eeg channel.\n",
        "\n",
        "The resulted $F_{1}$ scores are for each algorithm : \n",
        "*   Gradient Boosting classifier : 0.69\n",
        "*   Decision Tree with a Bagging classifier : 0.66.\n",
        "\n",
        "We observe an enhancement of the performance with the Gradient Boosting classifier as the leader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dazoQ-ZUwS2v",
        "colab_type": "text"
      },
      "source": [
        "### 6.2.3 Application of mutilple wavelet transforms to multiple eeg channels.\n",
        "Our next move is to apply the same multiple wavelts transform for mutliple eeg channels. Actually the data provided contains 7 eeg channels. we will use them all.\n",
        "\n",
        "The resulted $F_{1}$ scores are for each algorithm : \n",
        "*   Gradient Boosting classifier : 0.73\n",
        "*   Decision Tree with a Bagging classifier : 0.71"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFu43IxcxyA-",
        "colab_type": "text"
      },
      "source": [
        "# 7. Hyper-parameters tuning for the best algorithm : Gradient Boosting classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JIsCw4Vac85",
        "colab_type": "text"
      },
      "source": [
        "## 7.1 The tuned parameters with Python implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Kb7HDEz6qK",
        "colab_type": "text"
      },
      "source": [
        "We suggest to fine tune three parameters: \n",
        "\n",
        "*   n_estimators : Numbers of the modeled sequential trees\n",
        "*   max_depth : maximul depth of the tree.\n",
        "*   min_samples_split : Minimum number of samples in leaf\n",
        "\n",
        "The search for the optimum paarameters is performed by grid search. We use *GridSearchCV* from sklearn library. We fix the learning rate at the default value 0.1 we look firstly at the optimal number of estimators. The other paameters are either set intuitively or following recommanded values like 0.1 for the learning_rate and 0.8 for subsample. 25% of features as for max_features needed to make a split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPG4hNaXyr3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
        "from sklearn.model_selection import GridSearchCV  # Performing grid search\n",
        "param_test1 = {'n_estimators':range(20,81,10)}\n",
        "gsearch1 = GridSearchCV( estimator = GradientBoostingClassifier( learning_rate=0.1, min_samples_split=500, min_samples_leaf= 50,\n",
        "max_depth=8, max_features='sqrt', subsample = 0.8 , random_state = 10), param_grid = param_test1, scoring= 'accuracy' ,iid=False, cv = 10)\n",
        "gsearch1.fit(X_train ,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF7qYVal0tsG",
        "colab_type": "text"
      },
      "source": [
        "The optimal number of estimators is n_estimators = 80. We use this optimal parameter value to tune tree specific parameters : max_depth and min_samples_split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-YvUft0_LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
        "gsearch2 = GridSearchCV (estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators = 80 , max_features='sqrt'\n",
        ", subsample = 0.8, random_state=10), \n",
        "param_grid = param_test2, scoring = 'accuracy',iid=False, cv = 10)\n",
        "gsearch2.fit(X_train ,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8PcJucg1mVu",
        "colab_type": "text"
      },
      "source": [
        "The optimal values are  : max_depth : 15, min_samples_split : 800.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7aMo4Gh7hUm",
        "colab_type": "text"
      },
      "source": [
        "## 7.2 Results of the tuned algorithm.\n",
        "We manage finally, by tuning the Gradient Boosting Classifier, to achieve the $F_1$ score of 0.764."
      ]
    }
  ]
}